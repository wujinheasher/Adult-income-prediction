---
title: "Adult Income Predict"
author: "Yun Wu"
date: "1/8/2020"
output: pdf_document

---

# Introduction

The adult dataset is from the 1994 Census database. It is also known as “Census Income” dataset. this dataset can be found at http://files.grouplens.org/datasets/movielens/ml-10m.zip. This project is related to the course Data Science: Capstone from HarvardX's Data Science Professional Certificate. Thanks to Dr. Rafael Irizarry, I really learn something important to me.
The income project is predicting adult income via variables such as age, education, race, workplace, etc. In this project, I have to clean and reduce the dimension first, then used several machine learning algorithm and compared the accuracy. the purpose is to get maximum possible accuracy in prediction.
This report contains Definition of the project, dataset, Modelling Approach, result, disscussion, and conclusion.



# preprocess the dataset

The adult income dataset is automatically downloaded

[adult income dataset] https://archive.ics.uci.edu/ml/datasets/adult

[adult income dataset -file]https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data

## Download the dataset

```{r , echo=TRUE, message=FALSE, warning=FALSE}
library(tidyr)
library(tidyverse)
library(caret)
library(magrittr)
library(randomForest)
options(digits = 6)

dl <- tempfile()
download.file("https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data", dl)
adult_income <- read.table(dl, sep = ',', fill = F, strip.white = T) %>% set_colnames( 
  c('age', 'workclass', 'fnlwgt', 'education', 'education_num', 'marital_status', 'occupation', 'relationship', 'race', 'sex', 'capital_gain', 'capital_loss', 'hours_per_week', 'native_country', 'income'))

head(adult_income)
```

## Dimension reducing 

```{r, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
mean(adult_income$capital_gain == 0) 
mean(adult_income$capital_loss == 0)
mean(adult_income$native_country == "United-States")
```

I can observe that above 90% adult have zero capital_gain and capital_loss, and about 90% adult come from united states. Therefore,these three variables are skew. so I decide to delete them.
regarding to education, it means same as education_num, and relationship is same as marital status. Fnlwgt is not related to our goal. So I delete education, relationship, and fnlwgt. So far, I finish the dimension reducing.

```{r head, echo = FALSE}
adult <- adult_income %>% select(-education, -fnlwgt, -relationship, -capital_gain, -capital_loss, native_country )

head(adult)
  
```
## Clean dataset

### Trim workclass column

```{r head2, echo = FALSE}
table(adult$workclass)
  
```

The above summary of the subset shows that the variable of workclass has too many levels. I found 'Never-worked' and 'Without-pay' have a few data so I combine them to unknown;
combine federal-gov, state-gov, and local-gov levels to government. 
combine self-emp-inc and self-emp-not-inc to self-employed.

```{r summary, echo = FALSE}
levels(adult$workclass)[c(1, 4, 9)] <- "Unknown"
levels(adult$workclass)[c(5, 6)] <- "Self_Employed"
levels(adult$workclass)[c(2, 3, 6)] <- "Government"
table(adult$workclass)
```

### Trim occupation column

```{r head3, echo = FALSE}
table(adult$occupation)
  
```
There are too many levels here, but I can block the occupation into several groups:Blue-Collar, Professional, Sales, Service, and  White-Collar.

```{r head4, echo = FALSE}
levels(adult$occupation)[1] <- "Unknown"
levels(adult$occupation)[c(2, 5)] <- "White_Collar"
levels(adult$occupation)[c(4, 5, 6, 7, 14 )] <- "Blue_Collar"
levels(adult$occupation)[c(5, 6, 8, 10 )] <- "Service"
levels(adult$occupation)[c(6)] <- "Professional"
levels(adult$occupation)[c(1, 3)] <- "Unknown"
table(adult$occupation)
  
```

### Trim marital_status column

```{r head5, echo = FALSE}
table(adult$marital_status)
  
```

Block the marital_status into Divorced, married, seperated, single, and widowed.

```{r head6, echo = FALSE}
levels(adult$marital_status)[c(2, 3, 4)] <- "Married"
levels(adult$marital_status)[3] <- "Single"
table(adult$marital_status)
  
```


So far, I complete the data preprocess.
\pagebreak


# Methods and Analysis


## Data Analysis 

Explore the variables can help us to understand this dataset. 

### Explore workclass


```{r}
adult %>% group_by(workclass, income) %>% summarize(n = n()) %>% ggplot(aes(workclass, n, fill = income)) + geom_bar(stat = "identity")
```
From the figure, those who are self employed have the highest tendency of making greater than $50,000 a year.

### Explore education_num


```{r}
adult %>% group_by(education_num, income) %>% summarize(n = n()) %>% ggplot(aes(education_num, n, fill = income)) + geom_bar(stat = "identity")
```
The in group proportion of making greater than $50,000 a year increase as the years of education increases

### Explore occupation


```{r}
adult %>% group_by(occupation, income) %>% summarize(n = n()) %>% ggplot(aes(occupation, n, fill = income)) + geom_bar(stat = "identity")
```
Nearly half of Professional occupation makes greater than $50,000 a year, while that percentage is only 13% for Service occupation.

### Explore marital_status 


```{r}
adult %>% group_by(marital_status, income) %>% summarize(n = n()) %>% ggplot(aes(marital_status, n, fill = income)) + geom_bar(stat = "identity")
```
For those who are married, nearly half of them are making greater than $50,000 a year.


### Explore race 


```{r}
adult %>% group_by(race, income) %>% summarize(n = n()) %>% ggplot(aes(race, n, fill = income)) + geom_bar(stat = "identity")
```
White and Asian-Pacific Islander have high earning potentials – over 25% of the observations of these 2 races make above $50,000 annually.


## Modelling Approach

### create train_set and test_set 

```{r}
set.seed(1)

test_index <- createDataPartition(y = adult$age, times = 1, p = 0.2, list = FALSE)
train_set <- adult[-test_index,]
test_set <- adult[test_index,]
```

### Logistic regression

```{r}
fit_glm <- train(income ~ ., method = "glm", data = train_set) 
y_hat_glm <- predict(fit_glm, test_set)
Accuracy_glm <- confusionMatrix(y_hat_glm, test_set$income)$overall["Accuracy"]
Accuracy_results <- tibble(method = "logistic regression", Accuracy = Accuracy_glm)
print.data.frame(Accuracy_results)
```
The accuracy of the logistic regression on test_set is 

### Random forest classification

```{r}
set.seed(1)
my_control <- trainControl(method = "cv", number = 5)
fit_rf <- train(y = train_set[,10], x =train_set[,1:9], method = "rf", ntree = 1000, trControl = my_control, allowParallel = TRUE)
y_hat_rf <- predict(fit_rf, test_set)
Accuracy_rf <- confusionMatrix(y_hat_rf, test_set$income)$overall["Accuracy"]
Accuracy_results <- bind_rows(Accuracy_results,
                          tibble(method="Random forest",  
                                     Accuracy = Accuracy_rf))
```
The accuracy of the logistic regression on test_set is 



# Results

The Accuracy values are the following:

```{r rmse_results3, echo = FALSE}
print.data.frame(Accuracy_results) 
```

We therefore found the better method is random forest.

# Discussion

We can see the variables of workclass, marital_status are categories. So the classification model is better then regression.
```{r}
dim(adult)
```

Taking about the random forest, due to the high dimension which is 10 of columns and 32561 of rows, it spends a lot of time on processing. I can't do more research. for example I would should have research optimized the parameters such as 'mtry' and 'ntree'. I probably didn't have the best model.


# Conclusion

According to the model, sometimes we can adjust an adult's income just by his race, workclass, marital status, occupation, education, etc.
Because of the category variables, the regression is not good at it. On contrast, the classification method performs better.


\pagebreak